---
title: "modeling-basics"
author: "Vandrade"
date: "2024-07-08"
output: html_document
---

# TIDY MODELING WITH R

## Chapter 4 - 5

```{r librarys 1}
library(tidyverse)
library(tidymodels)

data(ames) 

```

```{r ames exploratory}

ames <- ames |> 
  mutate(Sale_Price = log10(Sale_Price))

ames |> 
  ggplot(aes(x=Sale_Price)) + 
  geom_histogram(col = 'white', bins = 50)

ames |> 
  filter(Lot_Area <= 50000) |> 
  ggplot(aes(x = Lot_Area)) + 
  geom_histogram(color = 'white', bins = 50)

ames |> 
  ggplot(aes(x = Year_Built)) + 
  geom_histogram(color = 'white', bins = 40)

```

### Spliting Train and Test data

#### Random - traditional - spliting
```{r traditional spliting}

set.seed(101)

ames_split <- ames |> 
  initial_split(.9)

ames_train <- ames_split |> 
  training()

ames_test <- ames_split |> 
  testing()

ames_train |> 
  ggplot(aes(x = Sale_Price)) +
  geom_density(color = 'red') +
  geom_density(data = ames_test, color = 'blue')

```

#### Stratified Spliting (maintaning the distribution of a series)

```{r Stratified Spliting}

set.seed(100)

ames_split <- ames |> 
  initial_split(prop = 0.9, strata = Sale_Price) 

ames_train <- ames_split |> 
  training()

ames_test <- ames_split |> 
  testing()

ames_train |> 
  ggplot(aes(x = Sale_Price)) +
  geom_density(color = 'red') +
  geom_density(data = ames_test, color = 'blue')

```

#### Spliting Validation and Training sets
```{r Spliting Validation}

set.seed(52) 

ames_val_split <- ames |> 
  initial_validation_split(prop = c(0.7, 0.15))

ames_train <- ames_val_split |> 
  training()

ames_validation <- ames_val_split |> 
  validation()

ames_test <- ames_val_split |> 
  testing()

```

## Chapter 6

### Fitting Models with Parsnip 

For tidymodels, the approach to specifying a model is intended to be more unified:

1 - Specify the type of model based on its mathematical structure (e.g., linear regression, random forest, KNN, etc).

2 - Specify the engine for fitting the model. Most often this reflects the software package that should be used, like Stan or glmnet. These are models in their own right, and parsnip provides consistent interfaces by using these as engines for modeling.

3 - When required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set.

These specifications are built without referencing the data. For example, for the three cases we outlined:


```{r}
library(tidymodels)

tidymodels_prefer()

linear_reg() |>
  set_engine("lm")
# computational engine can be: lm, glmnet or stan 

linear_reg() |> 
  set_engine("lm") |> 
  translate()

```

### House prices - long and lat

```{r}

lm_model <- linear_reg() |>
  set_engine("lm")

lm_form_fit <- lm_model %>% 
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

lm_form_fit |> tidy()
  
lm_xy_fit <- 
  lm_model %>% 
  fit_xy(
    x = ames_train %>% select(Longitude, Latitude),
    y = ames_train %>% pull(Sale_Price))

```

```{r}

rand_forest(trees = 1000, min_n = 5) %>% 
  set_engine("ranger") %>% 
  set_mode("regression") %>% 
  translate()

```















